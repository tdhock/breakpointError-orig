\documentclass{article}
\usepackage{fullpage}
\usepackage{graphicx}
%\documentclass{jsfds} %autoloads graphicx

% latin1 encoding is strongly adviced; 
\usepackage[latin1]{inputenc}
% If you are using utf8 encoding, please note that jsfds.sty and jsfds.cls have
% be converted into utf8 encoding as well
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\usepackage{amsmath,amsthm,amsfonts,amssymb}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
%\usepackage{dtklogos}-+
\usepackage{url}
\usepackage{natbib}

%\startlocaldefs
% Commandes auteurs spécifiques, par exemple :
\newcommand{\fig}[3][H]{
  \begin{figure}[#1]
    \hskip -1cm
    \input{figure-#2}
    \caption{#3}
    \label{fig:#2}
  \end{figure}
}
\newcommand{\figpdf}[3][H]{
  \begin{figure}[#1]
    \hskip -1cm
    \includegraphics[width=\textwidth]{figure-#2}
    \caption{#3}
    \label{fig:#2}
  \end{figure}
}
\newcommand{\tab}[3][H]{
  \begin{table}[#1]
    \begin{center}
          \input{table-#2}
    \end{center}
    \caption{#3}
    \label{tab:#2}
  \end{table}
}
\usepackage{tikz}
\usepackage{float}
%\usepackage{stfloats}
%\usepackage{morefloats}
\renewcommand{\r}{ \mathbf{ r} }
\newcommand{\rileft}[1][i]{\underline r_{#1}}
\newcommand{\riright}[1][i]{\overline r_{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\RR}{\mathbb{R}}
%\newcommand{\E}{\mbox{\tiny \`eme}}
%\def\1{1\kern-.20em {\rm l}}
%\endlocaldefs

%\setmainlanguageenglish
%\firstpage{1} % à régler par l'éditeur

\begin{document}

% TODO: UNCOMMENT frontmatter for publication!

%\begin{frontmatter}
  
  % \titre{A breakpoint detection error function for segmentation model
  % selection and evaluation}
  % \titrecourant{A breakpoint detection error function for segmentation model
  % selection and evaluation}

  % \begin{InfosAuteurs}
  %   \auteur{%
  %     \prenom{Toby Dylan} \nom{Hocking}%
  %     \thanksref{t1}%
  %     \contact[label=e1]{toby.hocking@mail.mcgill.ca}%
  %   }%
  %   \affiliation[t1]{McGill University Department of Human Genetics, 
  %     Montr\'eal, Canada.\\
  %     \printcontact{e1}%
  %   }
  %   \enteteauteurs{TD Hocking} %noms courants des auteurs en tête de page
  % \end{InfosAuteurs}

  \title{A breakpoint detection error function for segmentation model
    selection and evaluation}
  \author{Toby Dylan Hocking}

\maketitle

  \begin{abstract}
    We consider the multiple breakpoint detection problem, which is
    concerned with detecting the locations of several distinct changes
    in a one-dimensional noisy data series. We propose the
    breakpointError, a function that can be used to evaluate estimated
    breakpoint locations, given the known locations of true
    breakpoints. We discuss an application of the breakpointError for
    finding optimal penalties for breakpoint detection in simulated
    data. Finally, we show how to relax the breakpointError to obtain
    an annotation error function which can be used more readily in
    practice on real data. A fast C implementation of an algorithm
    that computes the breakpointError is available in an R package on
    R-Forge.
  \end{abstract}

\tableofcontents

%\end{frontmatter}

  % infos fournies par l'éditeur, laisser vide
  % \volume{}
  % \numero{}
  % \anneepublication{}
  % \doi{}
  %\arxiv{math.PR/00000000} % Si disponible

  
  % \begin{motscles}
  %   \mot{mode d'emploi}%
  %   \mot{classe du J-SFdS}%
  % \end{motscles}

  % \begin{motsclesalter}
  %   \mot{users guide}
  %   \mot{J-SFdS document class}
  % \end{motsclesalter}
  
  % \begin{AMSclass}
  %   \kwd{35L05}
  %   \kwd{35L70}
  % \end{AMSclass}
 
\newpage

\section{Introduction to segmentation models}

The goal of a segmentation model or algorithm is to divide a series of
data into distinct segments. A major application of segmentation
models is in detecting changes in copy number in cancer, using
technologies such as array comparative genomic hybridization
\citep{PSS98}. In these noisy biological data sets, the goal of
segmentation is to detect the precise base pairs or genomic positions after
which there are changes in copy number.

How to evaluate the accuracy of a segmentation model? A new method for
supervised segmentation of copy number data was proposed by
\citet{HOCKING-breakpoints}, who quantified the segmentation model
accuracy using an annotation database containing visually-determined
regions with or without breakpoints.
% This line of research was
% continued by \citet{HOCKING-penalties}, who proposed to use the
% annotated regions to learn a penalty function for optimal breakpoint
% detection. Finally, \citet{SegAnnDB} proposed a computer vision web
% site for interactive segmentation analysis using annotated
% regions. 
This method depends critically on the
definition of the visually-determined annotated region database, which
is used to compute an annotation error function. 

This paper continues
this line of research by defining the breakpointError function, which
uses the true breakpoints to precisely compute the accuracy of a
segmentation model. Also in this paper we demonstrate that the
breakpointError is closely related to the annotation error, thus
giving a theoretical foundation to the very practical new methods
based on visually-determined annotated region databases.

In this introduction, we first discuss a few motivating examples with
figures. In Section~2 we discuss related work, and in Section~3 we
define the breakpointError. In Section~4 we show an
application of the breakpointError, and in Section~5 we discuss its
relationship to the annotation error. In general we use bold to denote
vectors ($\mathbf x, \mathbf{\hat y}^k$) and plain text to denote
elements of those vectors ($x_i, y_i^k$) and scalars ($p, \hat
\sigma^2_k$).

\subsection{Definition of breakpoints}

Assume there are $P$ distinct positions in a series at
which data could be gathered. Let $\mathcal P= \{1,\dots,P\}$ be the
set of all such positions. For every position $p\in\mathcal P$, we
assume there is some true probability distribution $D_p$. Let $\mathbb
B=\{1,\dots,P-1\}$ be all bases after which a breakpoint is possible.

\begin{definition}
  A \textbf{breakpoint} is any position $p\in\mathbb B$ for which the
  next position does not have the same distribution: $D_p \neq
  D_{p+1}$. 
\end{definition}

For a series with $P$ positions, there is a minimum of 0 breakpoints
($D_1=\cdots=D_P$) and a maximum of $P-1$ breakpoints ($D_1 \neq
\cdots \neq D_P$). Note that the changes in distribution may be in
mean, variance, or any other parameters that affect the distribution.

The segmentation algorithm is given a sample of size $d \leq P$ of
data $(p_1, y_1), \dots, (p_d, y_d)$, with positions $p_i\in\mathcal
P$ and noisy observations $y_i\sim D_{p_i}$ for all samples $i\in\{1,
\dots, d\}$.

\newpage

For example, consider the normal distributions and simulated data
shown in Figure~\ref{fig:motivation}. The two panels show different,
separate segmentation problems. The top panel shows a problem with two
changes in mean, and the bottom panel shows two changes in
variance. For both panels in the figure, there are $P=500$ distinct
positions, $d=100$ simulated samples, and two breakpoints:
$\{300, 400\}$.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figure-motivation.pdf}
  \vskip -0.5cm
  \caption{Two signals with the same breakpoints (vertical dashed
    lines) but different distributional changes. Black circles show
    $d=100$ sampled data points drawn from normal distributions
    defined on positions $\mathcal P=\{1, \dots, 500\}$. Horizontal
    line segments and shaded bands show mean $\pm$ one standard
    deviation of the true normal distributions $D_p$. The goal of segmentation
    is to recover the distributions and/or breakpoints, using only the
    sampled data points.}
  \label{fig:motivation}
\end{figure}

\subsection{Maximum likelihood segmentation algorithms}
\label{sec:max-lik}

A segmentation algorithm takes the $d$ sampled data points as input,
and returns a list of estimated distributions and/or breakpoints. In
this section, we will review one class of segmentation algorithms
called maximum likelihood segmentation.

A maximum likelihood segmentation model for multiple breakpoints in
the mean of a normal distribution was proposed by
\citet{statistical-approach}. Let $\mathbf y = \left[
  \begin{array}{ccc}
    y_1 & \cdots & y_d
  \end{array}
\right]^\intercal \in \RR^d$ be the vector formed by the $d$
sampled data points, and let  $\mathbf p = \left[
  \begin{array}{ccc}
    p_1 & \cdots & p_d
  \end{array}
\right]^\intercal \in \mathcal P^d$ be the corresponding vector of
positions, ordered such that $p_1 < \cdots < p_d$. Then for any number
of segments $k\in\{1, \dots, d\}$, the estimated mean vector $\mathbf{\hat
  y}^k\in\RR^d$ is defined as
\begin{equation}
\label{eq:yhat^k}
\begin{aligned}
\mathbf{\hat  y}^k = &\argmin_{\mathbf x \in \RR^d} &&  ||\mathbf y - \mathbf x||^2_2
\\
&\text{subject to} && k-1=\sum_{j=1}^{d-1} 1_{x_j\neq x_{j+1}},
\end{aligned}
\end{equation}
where $||\mathbf x||^2_2=\sum_{j=1}^d x_j^2$ is the squared $\ell_2$
norm. Note that the optimization objective of minimizing the squared
error is equivalent to maximizing the Gaussian likelihood with uniform
variance \citep{statistical-approach}. For a fixed $k_{\text{max}}\leq
d$, we can quickly calculate $\mathbf{\hat y}^k$ for all
$k\in\{1,\dots,k_{\text{max}}\}$ using pruned dynamic programming
\citep{pruned-dp}. For any model size $k$, the estimated variance
$\hat \sigma^2_k\in\RR^+$ is defined as the mean of the squared
residuals:
\begin{equation}
  \label{eq:sigmahat}
  \hat \sigma_k^2 = ||\mathbf y - \mathbf{\hat y}^k||^2_2/d.
\end{equation}

The derivation is similar for the model of multiple breakpoints in the
variance of a normal distribution \citep{lavielle2005}, and can be
computed using the methods of \citet{pelt} or \citet{segmentor}. For
both models, we visually represent the true distribution and estimates
for $k\in\{2, \dots, 5\}$ in Figure~\ref{fig:motivation-modelOnly}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figure-motivation-modelOnly}
  \vskip -0.5cm
  \caption{Comparing true and estimated distributions is one approach
    to segmentation model selection/evaluation but is not the subject
    of this paper. \textbf{Top 2 panels}: the same reference/true
    signals as in Figure~\ref{fig:motivation}. \textbf{Others}:
    estimated maximum likelihood models $\mathbf{\hat y}^k,\hat
    \sigma^2_k$ for $k\in\{2, \dots, 5\}$ segments.}
  \label{fig:motivation-modelOnly}
\end{figure}

\subsection{Model selection}

The segmentation model selection problem may be posed as follows. Of
the 4 estimated segmentation models $k\in\{2, \dots, 5\}$, which is
the closest to the true model?

\newpage

One method for segmentation model selection is to compare the true
distribution with the estimated distributions
(Figure~\ref{fig:motivation-modelOnly}), and choose the estimate whose
distribution is closest to the true distribution. Assuming the true
probability distributions $D_p$ are available, one could compare them
with the estimates using a distance function such as the earth mover's
distance \citep{earth-mover}, or some other distance
function. However, the true distribution is not available in practice
on real data, so in this paper we will not explore segmentation model
selection via comparing distributions.

Instead, we propose a method for comparing the true and estimated
breakpoints. For any $d$-vectors of data and positions $(\mathbf x,
\mathbf p)$, we estimate the breakpoint locations using
%the position nearest to the center
\begin{equation}
  \label{eq:breaks_phi}
\phi(\mathbf{x}, \mathbf p)
= \big\{
\lfloor 
(p_j+p_{j+1})/2
\rfloor
\text{ for all }j\in\{1,\dots,d-1\}\text{ such that }
\mathbf x_j\neq \mathbf x_{j+1}
\big\}.
\end{equation}
Thus for any model size $k$, we estimate the breakpoint positions
using $\phi(\mathbf{\hat y}^k, \mathbf p)$.
In Figure~\ref{fig:motivation-breaksOnly}, we compare
these estimated breakpoints to the true set of breakpoints
\begin{equation}
  \label{eq:breaks_B}
  B = \{j\in\mathbb B:D_j\neq D_{j+1}\}.
\end{equation}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figure-motivation-breaksOnly}
  \vskip -0.5cm
  \caption{Same as Figure~\ref{fig:motivation-modelOnly}, but showing
    breakpoints instead of distributions. This paper proposes to
    compare the true and estimated breakpoints with the
    breakpointError function, which can be used for any reference/true
    distribution and any type of change. }
  \label{fig:motivation-breaksOnly}
\end{figure}

\newpage

Figure~\ref{fig:motivation-breaksOnly} clearly shows 3 distinct types
of errors that are possible in estimating the breakpoint positions:
\begin{description}
\item[False negative (FN)] for both data sets, the models with 2
segments are suboptimal because they only detect 1 of the 2 true
breakpoints.
\item[False positive (FP)] for both data sets, the models with 4
  segments are suboptimal since they detect 3 rather than 2
  breakpoints. The models with 5 segments are even worse since they
  detect 4 breakpoints.
\item[Imprecision (I)] of the two models with 3 segments, the
  breakpoints estimated for the change in variance data are more
  precise (closer to the true breakpoint positions).
\end{description}

This paper proposes the breakpointError function
(Figure~\ref{fig:motivation-breakpointError}), which can be used to
quantify these intuitive observations. The breakpointError can be
computed to quantify how well a set of estimated breakpoint positions
matches a true or reference set of breakpoints.


\begin{figure}[H]
  \centering
  \input{figure-motivation-breakpointError}
  \vskip -0.7cm
  \caption{For the same data shown in
    Figure~\ref{fig:motivation-breaksOnly}, we computed the
    breakpointError function (E) and its components. False positives
    (FP) occur when there are more estimated than true breakpoints,
    and false negatives (FN) are the opposite. For the correct model
    size (3 segments = 2 breakpoints), the imprecision function (I)
    quantifies the distance between the true and estimated breakpoint
    positions. The breakpointError is the sum of the other components
    (E=FP+FN+I).}
  \label{fig:motivation-breakpointError}
\end{figure}



\newpage

\section{Related Work}

This paper has been revised and expanded from Chapter~4 of the
doctoral thesis of \citet{HOCKING-phd-ch4}, which has not been
previously published elsewhere. Differences include minor changes
in notation, an expanded introduction, and more complete references.

The main subject of this paper is the breakpointError (defined in
Section~\ref{sec:definition}), which is a function for precisely
measuring the breakpoint detection accuracy of a segmentation
model. There are several other approaches for evaluating segmentation
models. \citet{zaid-lasso} compared the number of detected breakpoints
with the number of true breakpoints, ignoring the positions of the
breakpoints. A more precise method was proposed by
\citet{perf-eval-framework}, who checked if the detected breakpoints
appear in regions of arbitrary size around the true breakpoints. In
contrast, the breakpointError we propose in this paper has no
arbitrary region size parameter. \citet{group-fused} used exact
equality of the estimated and true breakpoint location in their
asymptotic theoretical analysis. The breakpointError function is more
precise since it is able to quantify that a guess close to a true
breakpoint is better than a guess far from a true breakpoint. A final
class of methods uses an annotated region database to quantify false
positive and false negative breakpoint detections
\citep{HOCKING-breakpoints, HOCKING-penalties}. An annotation database
can be created by drawing regions on scatterplots of the data using a
graphical user interface \citep{SegAnnDB}. Evaluating a segmentation
model via annotated regions is similar to the breakpointError function
we propose in this paper, and the precise link between these methods
will be explored in Section~\ref{sec:relaxation}.

Section~\ref{sec:simulations} shows one example application of the
breakpointError function, for determining the optimal form of penalty
functions in segmentation models for simulated data. Many related
penalties have been proposed for the change-point detection
problem. The standard AIC or BIC criteria are not well adapted in this
context since the model collection is exponential
\citep{BM04,BIC,Akaike73,BGH09}, and also because change-points are
discrete parameters \citep{mBIC}.  Many criteria specifically adapted
to change-point models have been proposed.  For example, there are
many different variants of the BIC \citep{Yao88,Lee95,mBIC}, and the
model selection theory of Birg\'e and Massart suggest other penalties
\citep{lavielle2005,lebarbier,BM04,calibration}. The precise
differences between these penalties and the penalties that we find
will be discussed in Section~\ref{sec:simulations}, but the main
difference is that the penalties discussed in this paper are
specifically designed to minimize the breakpointError (rather than
some other function, e.g. the squared error or negative log likelihood
of the data).
% These penalties are derived from
% theoretical considerations and give us insight into which terms are
% important for selecting a good segmentation model. The exact formula
% of these penalties depends on various assumptions such as normality
% and independence. These assumptions are often violated in real data,
% which can lead to selection of a suboptimal model. So in this chapter,
% rather than taking a particular penalty for granted, we propose to
% learn the penalty using annotation data.


\newpage

\section{Definition of the breakpointError}
\label{sec:definition}

Let us recall the notation of Section~1. Assume there are $P$ distinct
positions in a series at which data could be gathered. Depending on
the desired application, these positions could be indices in a data
vector, genomic positions, or time points. Let $\mathcal P=
\{1,\dots,P\}$ be the set of all such positions. For every position
$p\in\mathcal P$, we assume there is some true probability
distribution $D_p$. Let $\mathbb B=\{1,\dots,P-1\}$ be all bases after
which a breakpoint is possible, and let $B = \{p\in \mathbb B : D_p
\neq D_{p+1}\}$ be the set of true breakpoints.

The segmentation algorithm is given a sample of size $d \leq P$ of
data $(p_1, y_1), \dots, (p_d, y_d)$, with positions $p_i\in\mathcal
P$ and noisy observations $y_i\sim D_{p_i}$ for all samples $i\in\{1,
\dots, d\}$. The job of the segmentation algorithm is to return a
breakpoint guess $G\subseteq \mathbb B$. The object of this section is to
define the breakpointError $E^B_{\text{exact}}(G)$, which quantifies
the accuracy of the guess $G$ with respect to the true breakpoints
$B$.

\subsection{Desired properties of the breakpointError function}
\label{sec:desired-properties}

We would like the breakpointError function $E^B_{\text{exact}}:
2^{\mathbb B}\rightarrow \RR^+$ to satisfy the following properties:

\begin{itemize}
\item \textbf{(correctness)} Guessing exactly right costs nothing:
  $E^B_{\text{exact}}(B)=0$.
\item \textbf{(precision)} A guess closer to a real breakpoint is less
  costly:\\if $B=\{b\}$ and $0\leq i<j$, then
  $E^B_{\text{exact}}(\{b+i\})\leq E^B_{\text{exact}}(\{b+j\})$ and
  $E^B_{\text{exact}}(\{b-i\})\leq E^B_{\text{exact}}(\{b-j\})$.
% In
%   fact, we will see that when we know the exact breakpoint locations
%   in simulated signals, we can construct a cost function that verifies
%   the strict inequality $<$. In real data, we will use the annotation
%   error which only verifies the weak inequality $\leq$.
\item \textbf{(FP)} False positive breakpoints are
  bad: if $b\in B$ and $g\not\in B$, then $E^B_{\text{exact}} (\{b\}) <
  E^B_{\text{exact}} (\{b,g\})$.
\item \textbf{(FN)} Undiscovered breakpoints are bad:
  $b\in B\Rightarrow E^B_{\text{exact}}(\{b\}) < E^B_{\text{exact}} (\emptyset)$.
\end{itemize}

In the next section we define the breakpointError, which satisfies all
4 properties. 

% To compute the breakpointError, the true breakpoints $B$
% must be known.

% However, the true breakpoints $B$ are not often known in real data
% sets. One exception is the evaluation framework proposed by
% \citet{perf-eval-framework}, in which the breakpointError could be
% applied. For other real data sets where the true breakpoints are
% unknown, the breakpointError can not be used. However, in
% Section~\ref{sec:relaxation} we will discuss the annotation error,
% which is a relaxed version of the breakpointError that can be used in
% real data sets.

% \begin{itemize}
% \item When the latent signal is available in simulations, we can use
%   the exact breakpoint locations $B$ to define $E^B_{\text{exact}}$,
%   which satisfies all 4 properties.
% \item In real data, we do not know the exact breakpoint locations $B$,
%   but we can approximate them using regions $\hat R$ that visually
%   contain breakpoints. Assuming that there is exactly 1 region in
%   $\hat R$ for every breakpoint in $B$, we can define
%   $E^{\hat R}_{\text{complete}}$, which also satisfies all 4 properties.
% \item In real data, we may not want to assume that the regions $\hat
%   R$ contain all breakpoints $B$. So we define $E^{\hat
%     R,A}_{\text{incomplete}}$, which only satisfies these properties
%   if the annotations $A$ are consistent with the real breakpoints $B$.
% \item In real data, an imperfect annotator is making the database of
%   regions $\hat R$ and annotations $A$. So we define $E_{01}^{\hat
%     R,A}$ to limit the influence of each annotation.
% \end{itemize}

% We will define each of these breakpoint detection error functions in
% the next sections, then compare them in
% Figures~\ref{fig:variable-density-sigerr-small} and
% \ref{fig:variable-density-sigerr}.
%Section~\ref{sec:incomplete}.

\newpage

\subsection{Definition of the breakpointError function}
\label{sec:breakpoint_error}

In this section, we use the exact breakpoint locations $B=\{B_1,
\dots, B_n\}$ to define the breakpointError function.

We define the error of a breakpoint location guess $g\in\mathbb
B$ as a function of the closest breakpoint in $B$. So
first we put the breaks in order, by writing them as $B_1<\cdots<
B_n$. Then, we define a set of intervals
$R_B=\{\r_1,\dots,\r_n\}$ that form a partition of $\mathbb B$. For each
breakpoint $B_i$ we define the region
${\r}_i=[\rileft,\riright]\in\mathbb I \mathbb B$, where $\mathbb
I\mathbb B\subset 2^{\mathbb B}$ denotes the set of all intervals of
$\mathbb B$. We take the notation conventions from the interval
analysis literature \citep{intervals}.

We define the upper limit of region $i$ as
\begin{equation}
  \label{eq:R_i}
\riright
=
  \begin{cases}
    P-1 & \text{if } i=n \\
    \lfloor (B_{i+1}+B_i)/2 \rfloor & \text{otherwise}
  \end{cases}
\end{equation}
and
the lower limit as
\begin{equation}
  \label{eq:L_i}
  \rileft =
  \begin{cases}
    1 & \text{if } i=1 \\
    \riright[i-1]+1 & \text{otherwise}.
  \end{cases}
\end{equation}

The breakpoints $B_i$ and regions $\r_i$ are labeled for a small
signal in Figure~\ref{fig:exact_imprecision}.

\begin{figure}[H]
  \centering
  \input{figure-breakpoint-error-pieces}
  \vskip -0.5cm
  \caption{For a small signal with 2 breakpoints, and for breakpoints
    $i\in\{1, 2\}$, we plot the $\ell_i$ functions that measure the
    precision of a guess in $\mathbf r_i = [\underline r_i, \overline
    r_i]$.  The blue signal $\mathbf m\in\RR^{22}$ has 2 breakpoints:
    $B=\{4,14\}$. To emphasize the discrete nature of the data, N is
    drawn at each of the $P=22$ distinct positions at which data
    could be gathered.}
  \label{fig:exact_imprecision}
\end{figure}


Intuitively, if we observe a breakpoint guess $g\in \r_i$, then its
closest breakpoint is $B_i$. To define the best guess in each region,
we use piecewise affine functions $C_{\underline r,b,\overline
  r}:\RR\rightarrow[0,1]$ defined as follows:
\begin{equation}
  \label{eq:cLxR}
  C_{\underline r,b,\overline r}(g) =
  \begin{cases}
    0 & \text{if }g=b \\
    (b-g)/(x-\underline r) & \text{if } \underline r< g< b \\
    (g-b)/(\overline r-x) & \text{if } b< g< \overline r\\
    1 & \text{ otherwise}.
  \end{cases}
\end{equation}
For each breakpoint $i$ we measure the precision of a guess
$g\in\mathbb B$ using
\begin{equation}
  \label{eq:ell_i_exact}
  \ell_i(g)=C_{\rileft,B_i,\riright}(g).
\end{equation}
These piecewise affine functions are shown in
Figure~\ref{fig:exact_imprecision} for a small signal with 2
breakpoints. Note that there is some degree of arbitrary choice in the
definition of the $\ell_i$ functions. For example, properly defined
piecewise quadratic functions could also satisfy the
\textbf{precision} property desired of the breakpointError
(Section~\ref{sec:desired-properties}).

Now, we are ready to define the exact breakpointError of a set of
guesses $G\subseteq\mathbb B$.
First, let $G \cap\r$ be the subset of guesses $G$ that fall in
region~$\r$. 

Then, we define the false negative rate for region $\r$ as 
\begin{equation}
  \label{eq:FN_i}
  \text{FN}(G,\r) = 
  \begin{cases}
    1 & \text{if } G\cap\r = \emptyset\\
    0 & \text{otherwise}
  \end{cases}
\end{equation}
and the false positive rate for region $\r$ as
\begin{equation}
  \label{eq:FP_i}
  \text{FP}(G,\r) =
  \begin{cases}
    0 & \text{if }G\cap\r = \emptyset\\
    |G\cap\r|-1 &\text{otherwise}
  \end{cases}
\end{equation}
and the imprecision of the best guess in region $r$ as
\begin{equation}
  \label{eq:imprecision}
  I(G,\r,\ell) =
  \begin{cases}
    0 & \text{if } G\cap\r = \emptyset\\
    \min_{g\in G\cap\r} \ell(g) & \text{otherwise}.
  \end{cases}
\end{equation}
When there are no breakpoints, we have $B=\emptyset$ and
$R_B=\emptyset$. But we still would like to quantify the false
positives, so let $G\setminus\big( \cup R_B\big) $ be the set of
guesses $G$ outside of the breakpoint regions $R_B$. 

\begin{definition}
  \label{def:exact_breakpoint_cost}
  The \textbf{breakpointError} of set of breakpoint guesses $G$ with
  respect to the true breakpoints $B$ is the sum of the False
  Positive, False Negative, and Imprecision functions:

\begin{equation*}
  {E }_{\text{exact}}^B(G) =
  \big|G\setminus(\cup R_B)\big|
 + \sum_{i=1}^{|B|}\textrm{FP}(G,\r_i)+\textrm{FN}(G,\r_i)+I(G,\r_i,\ell_i).
\end{equation*}
\end{definition}

\subsection{Implementation}

To compute the exact breakpointError, we first sort lists of $n=|B|$
and $m=|G|$ items. Using the quicksort algorithm, this requires
$O(n\log n + m\log m)$ operations in the average case
\citep{clrs}. Once sorted, the components of the cost can be computed
in linear time $O(n + m)$. So, overall the computation of the error
can be accomplished in best case $O(n + m)$, average case $O(n\log n +
m\log m)$ operations. Its computation is implemented in efficient C
code in the \verb|breakpointError| R package on R-Forge, which can be
installed in R using

\begin{verbatim}
install.packages("breakpointError", repos="http://r-forge.r-project.org")
\end{verbatim}


\newpage

\section{Penalties with minimal breakpointError in simulations}
\label{sec:simulations}
In this section, we show several examples of how to use the
breakpointError function to determine penalties which minimize the
train and test breakpointError in simulated data sets. In all cases,
we will assume that there is a database of several piecewise constant
signals with Gaussian noise. The goal is to learn a penalty constant
that can be shared between signals with different properties. In each
of the following sections, we will first present an empirical analysis
of several simulated signals using the breakpointError. Then, we will
discuss the relationship of our results to relevant theoretical
results.

\subsection{Sampling density normalization}
\label{variable_density}
% Having properly defined how to compute the breakpointError in
% Section~\ref{sec:breakpoint_error}, we now use it to derive several
% results about optimal penalties for breakpoint detection. 
The first problem we consider is finding a penalty that is invariant
to sampling density. This is important because sampling density is
often not uniform in real data sets. In fact, we see a sampling
density between 40 and 4400 kilobases per probe in the neuroblastoma
data set of \citet{HOCKING-breakpoints}. We would like to construct a
single algorithm or penalty function that can be used for each of
these segmentation problems.

So to determine the form of the penalty function that can best adapt
to this variation, we analyze the following simulation. We create a
true piecewise constant signal $\mathbf m\in\RR^P$ over $P=70000$ base
pairs, with breakpoints every 10000 base pairs, shown as the blue line
in Figure~\ref{fig:variable-density-signals}. Then, we define a signal
sample size $d_i\in\{70,\dots,70000\}$ for every noisy signal
$i\in\{1,\dots,z\}$. Let $y_i\in\RR^{d_i}$ be noisy signal $i$,
sampled at positions $p_i\in\mathcal P^{d_i}$, with
$p_{i1}<\cdots<p_{i,d_i}$. We sample every probe $j$ from the
$y_{ij}\sim N(m_{p_{ij}},1)$ distribution. These samples are shown as
the black points in Figure~\ref{fig:variable-density-signals}.

We would like to learn some model complexity parameter $\lambda$ on
the first noisy signal, and use it for accurate breakpoint detection
on the second noisy signal. In other words, we are looking for a model
selection criterion which is invariant to sampling density. 

\begin{figure}[H]
\includegraphics[width=\linewidth]{figure-variable-density-signals}
%\input{figure-variable-density-signals}
\vskip -0.3cm
  \caption{Two noisy signals (black) sampled from
  a true piecewise constant signal (blue). 
Note that these are the same signals that appear in
  Figure~\ref{fig:variable-density-annotation-cost}.}
\label{fig:variable-density-signals}
\end{figure}

\newpage


To attack this problem, we proceed as follows. For every signal $i$,
we use pruned dynamic programming to calculate the maximum likelihood
estimator $\mathbf{\hat y}^k_i\in\RR^{d_i}$ (\ref{eq:yhat^k}), for several model sizes
$k\in\{1,\dots,k_{\text{max}}\}$ \citep{pruned-dp}. Then, we define
the model selection criteria
\begin{equation}
  \label{eq:kstar_density}
  k^\alpha_i(\lambda) =\argmin_k \lambda k d_i^\alpha + 
  ||\mathbf{y_i}-\mathbf{\hat y}^k_i||_2^2.
\end{equation}
Each of these is a function $k_i^\alpha:\RR^+\rightarrow
\{1,\dots,k_{\text{max}}\}$ that takes a model complexity tradeoff
parameter $\lambda$ and returns the optimal number of segments for
signal $i$. The goal is to find a penalty exponent $\alpha\in\RR$ that
lets us generalize $\lambda$ between different signals $i$.


To quantify the accuracy of a segmentation for signal $i$, let
$\text{BErr}_i(k)$ be the breakpointError of the model with
$k$ segments. This is a function
$\text{BErr}_i:\{1,\dots,k_{\text{max}}\}\rightarrow\RR^+$, defined as
\begin{equation}
  \label{eq:berr}
  \text{BErr}_i(k) = E_{\text{exact}}^{B}\left[
\phi(\mathbf{\hat y}_i^k,p_i)
\right].
\end{equation}
where $B$ is the set of real breakpoints in the true piecewise constant
signal $\mathbf m$.

In Figure~\ref{fig:variable-density-berr-k}, we plot $\text{BErr}_i$
for the 2 simulated signals $i$ shown previously.  As expected, the
model recovers more accurate breakpoints from the signal sampled at a
higher density.  

\fig{variable-density-berr-k}{Exact breakpoint error
  $\text{BErr}_i(k)$ for two signals $i$ and several model
  sizes $k$. Note that these are the same error curves that appear
in the Breakpoint panels of Figure~\ref{fig:variable-density-sigerr-small}.}

\newpage

Now, let us define the penalized
model breakpoint error $E^\alpha_i:\RR^+\rightarrow\RR^+$ as
\begin{equation}
  \label{eq:lerr}
E^\alpha_i(\lambda) = \text{BErr}_i\left[
k^\alpha_i(\lambda)
\right].
\end{equation}
In Figure~\ref{fig:variable-density-berr}, we plot these functions for the
two signals $i$ shown previously, and for several penalty exponents $\alpha$.

The dots in Figure~\ref{fig:variable-density-berr} show the optimal
$\lambda$ found by minimizing the penalized model breakpoint detection
error:
\begin{equation}
  \label{eq:lambda_hat}
  \hat \lambda^\alpha_i = \argmin_{\lambda\in\RR^+}  E^\alpha_i(\lambda)
\end{equation}

Figure~\ref{fig:variable-density-berr} suggests that $\alpha\approx1/2$
defines a penalty with aligned error curves, which will result in
$\hat \lambda_i^\alpha$ values that can be generalized between
profiles. 
%Next, we will attempt to determine the optimal $\alpha$
%value.

\fig{variable-density-berr}{Model selection error curves
  $E_i^\alpha(\lambda)$ for 2 signals $i$ and several exponents
  $\alpha$. The penalty contains a term for the number of points sampled $d_i^\alpha$.}

\newpage

Now, we are ready to define 2 quantities that will be able to help us
choose an optimal penalty exponent $\alpha$.

First, let us consider the training error over the entire database:
\begin{equation}
  \label{eq:lerr_train}
  E^\alpha(\lambda) = \sum_{i=1}^z E_i^\alpha(\lambda),
\end{equation}
and we define the minimal value of this function as
\begin{equation}
  \label{eq:lerr_train_min}
  E^*(\alpha) = \min_\lambda E^\alpha(\lambda).
\end{equation}
In Figure~\ref{fig:variable-density-error-train}, we plot these
training error functions $E^\alpha$ and their minimal values $E^*$ for
several values of $\alpha$. It is clear that the minimum training
error is found for some penalty exponent $\alpha$ near 1/2, and we
would like to find the precise $\alpha$ that results in the lowest
possible minimum $E^*(\alpha)$.

\fig{variable-density-error-train}{Training error functions $E^\alpha$
  in black and their minimal values $E^*(\alpha)$ in red. The penalty
  contains a term for the number of points sampled $d_i^\alpha$.}

\newpage

We also consider the test error over all pairs of signals when
training on one and testing on another:
\begin{equation}
  \label{eq:lerr_test}
  \text{TestErr}(\alpha) = 
  \sum_{i\neq j} E^\alpha_i(\hat \lambda_j^\alpha).
\end{equation}

In Figure~\ref{fig:variable-density-error-alpha}, we plot $E^*$ and
TestErr for a grid of $\alpha$ values.  It is clear that the optimal
penalty is given by $\alpha=1/2$. This corresponds to the following
model selection criterion which is invariant to the number of data
points sampled $d_i$ (for different simulated signals $i$ with the
same true breakpoints):
\begin{equation}
  \label{eq:var_dens_opt_pen}
  k_i(\lambda) = 
  \argmin_k \lambda k \sqrt{d_i}+||\mathbf y_i-\mathbf{\hat y}_i^k||^2_2
\end{equation}

\fig{variable-density-error-alpha}{Train and test breakpoint detection
  error as a function of penalty exponent $\alpha$. The penalty
  contains a term for the number of points sampled $d_i^\alpha$. Mean
  error is drawn as a black line, with one standard deviation shown as
  a grey band.}

\newpage

As explained by \citet{sylvain-survey}, a model selection procedure
can be either efficient or consistent. An efficient procedure for
model estimation accurately recovers the true piecewise constant signal, whereas a
consistent procedure for model identification accurately recovers the
breakpoints. Since we attempt to minimize the breakpointError, we are
attempting to construct a consistent penalty, not an efficient
penalty.

In general terms, the fact that we find a nonzero exponent $\alpha$
for our $d_i^\alpha$ penalty term agrees with other results. In
particular, \citet{vfold} proposed an optimal procedure to select model
complexity parameters in cross-validation by normalizing by the sample
size $d_i$. 
% This is in
% intuitive agreement with our result that we need to normalize by the
% sample size $d_i$, although they do not use the square root term
% $\sqrt{d_i}$.

The $\sqrt{d_i}$ term that we find here using simulations is in
agreement with \citet{aurelie}, who used finite sample model selection
theory to find a $\sqrt{d_i}$ term in a penalty optimal for
clustering.

When theoretically deriving an efficient penalty for segmentation
model estimation in the non-asymptotic setting, \citet{lebarbier}
obtained a $\log d_i$ term. This contrasts our result, which attempts
to find a consistent penalty, and uses the breakpointError to find a
$\sqrt{d_i}$ penalty term. But in fact this is in agreement with
classical results that the efficient AIC underpenalizes with respect
to the consistent BIC, as shown in Table~\ref{tab:AIC-BIC}.

\begin{table}[H]
  \centering
  \begin{tabular}{cc|cc}
     Efficient & Penalty & Consistent & Penalty \\
     Model & Term & Model & Term\\
     \hline
     AIC & 2 & BIC & $\log d_i$\\
     Lebarbier & $\log d_i$ & This work & $\sqrt{d_i}$\\
  \end{tabular}
  \caption{Comparing our results with Lebarbier, 
in the context of classical results involving AIC and BIC. 
The BIC is designed for model identification and penalizes more than the AIC.
Likewise, our penalty examines model identification using the breakpoint
detection error, and penalizes more than the efficient penalty proposed
by Lebarbier.}
  \label{tab:AIC-BIC}
\end{table}



\newpage
\subsection{Signal length normalization}
\label{variable_size}
In real array CGH data, we need to analyze chromosomes of varying length
in base pairs. For example, human chromosome 1 is the largest at about
250 mega base pairs, and chromosome 22 is the smallest with only about
36 mega base pairs. But we expect that the number of breakpoints is
proportional to the length of the chromosome in base pairs, and we would
like to design a model selection criterion that is invariant to the
signal length.

So as a first step toward constructing a penalty that is invariant to
the number of breakpoints, we consider the following simulation where
we fix the number of points sampled at $d_i=2000$, and vary the length
of the signal sampled. In
Figure~\ref{fig:variable-breaks-constant-size}, we show samples of 2
different lengths $l_i$, for the same true piecewise constant signal
$\mathbf m$. This simulation is somewhat unrealistic since the number
of data points $d_i$ in real data sets is usually proportional to the
signal length $l_i$. We will consider a more realistic simulation
model and a more complicated penalty in the next section.

\figpdf{variable-breaks-constant-size}{Samples of 2 different lengths
  $l_i$ but constant number of points $d=2000$.}

\newpage

For each signal $i$, we define the penalty
\begin{equation}
  \label{eq:kstar_length}
  k_i^\beta(\lambda) = \argmin_k \lambda k l_i^\beta
+ ||\mathbf y_i - \mathbf{\hat y}_i^k||^2_2,
\end{equation}
where $l_i$ is the length of the signal in base pairs. The goal will
be to find a $\beta$ that can be used for signals of varying length.


In Figure~\ref{fig:variable-breaks-constant-size-berr}, we show
the breakpoint detection error curves for two signals and several
penalty exponents $\beta$.
These curves seem to align when $\beta=-1/2$.

\fig{variable-breaks-constant-size-berr}{Breakpoint detection error
  curves for several penalty exponents $\beta$ and 2 samples of
  varying length in base pairs $l_i$. The penalty contains a term
  $l_i^\beta$.}

\newpage

In Figure~\ref{fig:variable-breaks-constant-size-alpha}, we plot the
train and test error curves over the entire set of simulated signals.
These curves indicate minimal breakpoint detection error at
$\beta=-1/2$, corresponding to the following penalty:
\begin{equation}
  \label{eq:kstar_length_opt}
  k_i(\lambda) = \argmin_k \frac{\lambda k}{\sqrt{l_i}}
  + ||\mathbf y_i-\mathbf{\hat y}_i^k||^2_2.
\end{equation}




\fig{variable-breaks-constant-size-alpha}{Train and test error curves
  for signals of different length in base pairs $l_i$. The penalty
  contains a term
  $l_i^\beta$.}


Interestingly, the $1/\sqrt{l_i}$ term that we obtain here is in good
agreement with our previous result that the optimal penalty for
variable sampling density $d_i$ should have a $\sqrt{d_i}$ term. In
particular, we can re-parameterize the problem to be in terms of the
number of points sampled per segment $\rho_i=d_i/k_i$. In
Section~\ref{variable_density} we held $k_i$ constant but in this
section we hold $d_i$ constant. In both cases we have a penalty with a
$\sqrt{\rho_i}=\sqrt{d_i/k_i}$ term.

However, we do not know the number of segments $k_i$ in advance. But
we supposed that the number of segments is proportional to the number
of base pairs $l_i$, so we can use that in the penalty. This suggests
a penalty that takes the form of $\sqrt{d_i/l_i}$. So in the next
section, we confirm that this intuition works for constructing an
optimal penalty.


\newpage
\subsection{Combining normalizations}
\label{combining_penalties}
In this section, we show that we can combine the results of the
previous sections to create composite invariant penalties. In
particular, to normalize for sampling density $d_i$ and length in base
pairs $l_i$, we need $\sqrt{d_i}$ and $1/\sqrt{l_i}$ terms in the
penalty, respectively. This suggests that when considering variable
$d_i$ and $l_i$, we need a $\sqrt{d_i/l_i}$ term in the penalty, and
in this section we show that this intuitive construction results in an
optimal penalty.

In Figure~\ref{fig:variable-size-signals}, we plot 2 signals with
different number of points $d_i$ and length in base pairs $l_i$. In
particular we tested $d_i\in\{50, \dots, 1000\}$ and $l_i\in\{200,
\dots, 1000\}$. We would like to find a penalty that allows us to
generalize model complexity tradeoff parameters $\lambda$ between
these signals.

For each signal $i$, we define the penalty
\begin{equation}
  \label{eq:kstar_composite}
  k_i^{\alpha,\beta}(\lambda) = \argmin_k \lambda k {d_i}^\alpha l_i^\beta
  + ||\mathbf y_i - \mathbf{\hat y}_i^k||^2_2,
\end{equation}
where $l_i$ is the signal length in base pairs and $d_i$ is the number
of points sampled. We will attempt to determine a pair of $\alpha$ and
$\beta$ values that allow accurate breakpoint detection in signals of
varying length and number of points sampled. Based on the results in
Sections~\ref{variable_density} and \ref{variable_size}, we expect to
find $\alpha=1/2$ and $\beta=-1/2$.

\begin{figure}[H]
  \centering
\includegraphics[width=\linewidth]{figure-variable-size-signals}
  \caption{Two signals with a different number of
    points $d_i$ and length in base pairs $l_i$.}
\label{fig:variable-size-signals}
\end{figure}

\newpage

In Figure~\ref{fig:variable-size-error-alpha-beta}, we plot the train
and test breakpoint error functions as a function of both $\alpha$ and
$\beta$. It is clear that the minimum is achieved by penalties near
$\alpha=1/2, \beta=-1/2$, which corresponds to a penalty of

\begin{equation}
  \label{eq:kstar_composite_values}
  k_i^{\alpha,\beta}(\lambda) = \argmin_k \lambda k
  \sqrt{d_i/l_i}
  + ||\mathbf y_i - \mathbf{\hat y}_i^k||^2_2,
\end{equation}

\figpdf{variable-size-error-alpha-beta}{Train and test error functions
  for several signals of varying number of points $d_i$ and length
  $l_i$. The penalty contains a term $d_i^\alpha l_i^\beta$. Mean
  error values normalized to $[0,1]$, minimum values indicated in red,
  and expected value $\alpha=1/2, \beta=-1/2$ in white. }

% \newpage

% In Figure~\ref{fig:variable-size-berr}, we plot the breakpoint error
% functions $E_i^\alpha$ for 2 signals $i$ and several exponents
% $\alpha$. The curves seem to align when $\alpha=-1/2$.

% \fig{variable-size-berr}{Breakpoint error functions $E_i^\alpha$ for
%   several exponents $\alpha$ and 2 signals $i$ of varying number of
%   points $d_i$ and length in base pairs $l_i$. The penalty contains a
%   term $l_i^\alpha\sqrt{d_i}$.}

% In Figure~\ref{fig:variable-size-error-alpha}, we plot the train and
% test error as a function of penalty exponent $\alpha$. This analysis
% suggests that the optimal exponent is $\alpha=-1/2$, as expected from
% our previous analysis in Section~\ref{variable_size}.

% \fig{variable-size-error-alpha}{Train and test error functions for
%   several signals of varying number of points $d_i$ and length in base
%   pairs $l_i$. The penalty contains a term $l_i^\alpha \sqrt{d_i}$.}

\newpage
\subsection{Optimal penalties for the fused lasso signal approximator}

In the previous sections, we used theoretical arguments and simulation
experiments to determine the optimal penalties for maximum likelihood
segmentation (\ref{eq:yhat^k}). In this section, we demonstrate that
the same approach can be used to find optimal penalties for another
model, the Fused Lasso Signal Approximator (FLSA).

We used the \verb|flsa| function in version 1.03 of the flsa
package from CRAN to calculate the FLSA \citep{fused-lasso-path}. Let
$\mathbf x\in\RR^d$ be the noisy copy number signal for one chromosome. The
FLSA solves the following optimization problem:
\begin{equation}
  \label{eq:flsa}
\argmin_{\mathbf m\in\RR^d} 
\frac 1 2 \sum_{j=1}^d (x_j-m_j)^2
+\lambda_1\sum_{j=1}^d|m_j|
+\lambda_2\sum_{j=1}^{d-1}|m_j-m_{j+1}|.
\end{equation}

First, we take $\lambda_1=0$ since we are concerned with breakpoint
detection, not signal sparsity. In this section, our aim is to
determine a parameterization for $\lambda_2$ that we will be able to
find similar breakpoints in signals of varying sampling density.

We use the same setup that we used to determine optimal penalties for
maximum likelihood segmentation, as described in
Section~\ref{variable_density} and shown again in
Figure~\ref{fig:variable-density-signals-flsa}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figure-variable-density-signals}
  \caption{Simulated signals with different
  sampling density.}
  \label{fig:variable-density-signals-flsa}
\end{figure}

\newpage

In particular, for every signal $i\in\{1,\dots,z\}$, let
$\mathbf y_i\in\RR^{d_i}$ be the noisy data, sampled at positions
$\mathbf p_i\in\mathcal P^{d_i}$. To find an optimal penalty for these data,
first let $\lambda_2 = \lambda d_i^\alpha$. For each signal $i$,
exponent $\alpha\in\RR$, and tradeoff parameter $\lambda\in\RR^+$, we
define the optimal smoothing as
\begin{equation}
  \label{eq:flsa_lambda}
  \mathbf{\hat y}^{\lambda,\alpha}_i = 
\argmin_{\mathbf m\in\RR^{d_i}} 
\frac 1 2 ||\mathbf y_i-\mathbf m||_2^2
+\lambda d_i^\alpha \sum_{j=1}^{d_i-1} |m_j - m_{j+1}|.
\end{equation}
% \begin{equation}
%   \label{eq:flsa_lambda}
%   \hat y^{\lambda,\alpha}_i = 
% \argmin_{\mu\in\RR^{d_i}} 
% \frac 1 2 ||y_i-\mu||_2^2
% +\lambda d_i^\alpha \TV_i(\mu),
% \end{equation}
% with the total variation $\TV_i:\RR^{d_i}\rightarrow \RR^+$ defined as
% \begin{equation}
%   \label{eq:TV}
%   \TV_i(x) = \sum_{j=1}^{d_i-1} |x_j - x_{j+1}|.
% \end{equation}

Then, we define the breakpoint detection error as a function of the
breaks in the smoothed signal:
\begin{equation}
  \label{eq:flsa_e_i_alpha}
  E_i^\alpha(\lambda) = 
E^B_{\text{exact}}
\left[
\phi\left(
\mathbf{\hat y}^{\lambda,\alpha}_i, \mathbf p_i
\right)
\right],
\end{equation}
where the breakpoint function $\phi$ is defined in
(\ref{eq:breaks_phi}).

We plot $E_i^\alpha$ for 2 signals $i$ and several penalty exponents
$\alpha$ in Figure~\ref{fig:variable-density-berr-flsa}. Note that the
functions appear to align when $\alpha=1$.

\fig{variable-density-berr-flsa}{Model complexity breakpoint error
  functions $E_i^\alpha$.}

\newpage

To evaluate which penalty parameter $\alpha$ results in optimal
fitting and learning, we computed train error $E^*$ and TestErr as defined
in (\ref{eq:lerr_train}) and (\ref{eq:lerr_test}). These functions are
plotted in Figure~\ref{fig:variable-density-error-alpha-flsa}, and
suggest that a value of $\alpha=1$ is optimal. This analysis suggests
that taking $\lambda_2=\lambda d_i$ is optimal for breakpoint
detection using FLSA. This agrees with the observation of
\citet{HOCKING-breakpoints} that the flsa.norm penalty with a $d_i$
term works better than the un-normalized flsa penalty.

However, we obtained a different penalty ($\alpha=0.5$) in
Section~\ref{variable_density} for another model, maximum likelihood
segmentation. These differences in optimal $\alpha$ values are due to
the differences in how model complexity is measured in the two
models. Maximum likelihood segmentation measures model complexity
using the $\ell_0$ pseudo-norm of the difference vector of $\mathbf m$,
whereas the FLSA uses the $\ell_1$-norm.

We conclude by noting that this procedure could also be applied to
find penalties for FLSA that depend on other signal properties such as
length in base pairs~$l_i$. However, we did not pursue this since FLSA
does not work as well as maximum likelihood segmentation in practice
on real data \citep{HOCKING-breakpoints}.

\fig{variable-density-error-alpha-flsa}{Train and test error as a
  function of penalty exponent $\alpha$. The penalty has a term for
  the number of points sampled $d_i^\alpha$.}


\newpage

\subsection{Applying the penalties to real data}

In Sections~\ref{variable_density}-\ref{variable_size}, we found
penalties with minimum breakpointError for simulated data with varying
number of data points sampled $d_i$ and length $l_i$ in base positions
(with $l_i$ proportional to the number of breakpoints). In
Section~\ref{combining_penalties}, we demonstrated that these results
can be combined. We also found that the optimal penalty should include
a term for the estimated variance $\hat s_i^2$
\citep{HOCKING-phd-ch4}. These results suggested the following
penalty, for every signal $i$:
\begin{equation}
  \label{eq:composite_penalty}
  k_i(\lambda) = 
  \argmin_k
  \lambda k \hat s_i^2 \sqrt{d_i/l_i}  +
  ||\mathbf y_i - \mathbf{\hat y}_i^k||^2_2
\end{equation}

In Table~\ref{tab:penalty-real-data}, we report results of using the
suggested penalties on the neuroblastoma data set described by
\citet{HOCKING-breakpoints}. The cghseg.k penalty which was found to
be the best by \citet{HOCKING-breakpoints} has a term for number of
data points sampled $d_i$ (no square root) but no terms for length
$l_i$ nor estimated variance $\hat s_i$. The penalty terms suggested
in this section do not improve breakpoint detection error in the
neuroblastoma data set. This observation suggests that distribution
that generates the real data is more complex than the simple
simulation model considered in this paper.

\tab{penalty-real-data}{Breakpoint detection error of several
  penalties on the neuroblastoma data set, with 1 row for each
  penalty. The exponent of the number of data points $d_i$, length
  $l_i$, and variance $\hat s_i$ terms in the penalty is shown with
  the train and test annotation error (percent incorrect regions). }

Practically speaking, we still would like to find a penalty with
optimal breakpoint detection for any particular real data set such as
the neuroblastoma data. 
\citet{HOCKING-penalties} achieved state-of-the-art breakpoint
detection in the neuroblastoma data set by learning
the penalty constants using a training data set of manually annotated
regions. 
%These annotation-guided techiniques have proven to yield
%accurate penalties for breakpoint detection in real data. 
For the rest
of this paper, we will discuss the relationship of the breakpointError
to these annotation-guided methods.

\newpage

\section{Annotation error functions for real data sets}
\label{sec:relaxation}

In this section, we define several annotation error functions which
can be used in real data sets (Table~\ref{tab:ann-err-funs}).  In real
data, we do not have access to the true piecewise constant signal
$\mathbf m$, nor the underlying set of breakpoints $B$. So the
breakpointError defined in the Section~3 is not readily
computable. We will first show how in real data, we can compute
another function called the incomplete annotation error. Then, we will
demonstrate its relationship to the breakpointError using the complete
annotation error function.

\begin{table}[H]
  \begin{center}
  \begin{tabular}{ccccc}
    Section & Error function & Symbol & Need & counts incorrect \\
    \hline
    \ref{sec:breakpoint_error}& breakpointError & $E^B_{\text{exact}}$ & 
    true breakpoints $B$ & guesses \\
    \ref{sec:incomplete} & incomplete annotation error & $E_{\text{incomplete}}^A$ &
    some annotations $A$ & guesses\\
    \ref{sec:complete} & complete annotation error & $E_{\text{complete}}^A$ & 
    all breakpoint annotations $A$ & guesses\\
    \ref{sec:zero-one}& 
    01 annotation error & $E_{01}^A$ & some annotations $A$ & regions
  \end{tabular}
  \end{center}
  \caption{Several breakpoint detection error functions, 
    and how much prior knowledge is needed to compute each. 
    The breakpointError needs the most prior knowledge and can only be 
    used in simulations when the true breakpoints $B$ are known. 
    In contrast, the incomplete/01 annotation error can be used in
    real data sets by using visual inspection of scatterplots to
    create annotations $A$.}
  \label{tab:ann-err-funs}
\end{table}

\subsection{Incomplete annotation error for real data}
\label{sec:incomplete}

By plotting a real data set, we can easily identify regions that
contain breakpoints by visual inspection, as shown in
Figure~\ref{fig:variable-density-annotation-cost}.

\begin{figure}[H]
  \centering
\includegraphics[width=\linewidth]{figure-variable-density-annotation-cost}
%\input{figure-variable-density-annotation-cost}
\vskip -0.1in
  \caption{\textbf{Top}: simulated noisy
  signals (black) with their true piecewise constant signals $\mathbf m$ (blue) 
  and
  visually-determined breakpoint annotations $A$
  (red). 
\textbf{Middle}: negative annotations $A^0$ constructed
  using (\ref{eq:A^0}).
\textbf{Bottom}: breakpoint detection
  imprecision curves for the breakpointError $\ell_i$
  (\ref{eq:imprecision}) and the annotation error $\hat
  \ell_i$ (\ref{eq:hat_ell_i}).}
\label{fig:variable-density-annotation-cost}
\end{figure}


%\newpage

Recall that there are $P$ distinct positions in a series at which data
could be gathered, 
%that $\mathcal P= \{1,\dots,P\}$ is the set of all such positions, 
and that $\mathbb B=\{1,\dots,P-1\}$ is the set of all
positions after which a breakpoint is possible.

\begin{definition}
  A set of $n$ \textbf{annotations} can be written as $A=\{(a_1,
  \r_1), \dots, (a_n, \r_n)\}$. For each annotation~$i$,
  $\r_i\subset\mathbb I\mathbb B$ is an interval that defines the
  region, and $a_i\subseteq\{0,1,\dots\}$ is an interval of allowable
  breakpoint counts in this region.
\end{definition}

For
example, consider the annotated regions in
Table~\ref{tab:sample_annotations}.

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ccc}
  $i$ & Allowed breakpoints $a_i$ & Region $\r_i$\\
\hline
1 & \{0\} & [5,10]\\
2 & \{1\} & [20,30]\\
3 & \{1,2,\dots\} & [40,70]\\
4 & \{0\} & [80,100]
\end{tabular}
  \end{center}
  \caption{Sample annotated regions for a signal sampled on $P=100$ base pairs. 
    An annotation $a_i$ indicates how many breakpoints are allowed in the
    corresponding region $\r_i$.
    There are 0 breaks in bases 5-10 and 80-100.
    There is exactly 1 break in bases 20-30.
    There is at least 1 break in bases 40-70.}
  \label{tab:sample_annotations}
\end{table}

Given a set of breakpoint guesses $G\subseteq \mathbb B$, we define
the annotation-dependent false positive count as
\begin{equation}
  \label{eq:FP_hat}
  \hat{ \text{FP}}(G,\r,a) =
    \big( 
|G\cap\r|-\max(a)
\big)_+
\end{equation}
where the positive part function is defined as
\begin{equation}
x_+ =
  \begin{cases}
    x & \text{ if } x > 0 \\
    0 & \text{ otherwise.}
  \end{cases}
\end{equation}
Similarly, the annotation-dependent false negative count is defined as
\begin{equation}
  \label{eq:FN_hat}
  \hat{ \text{FN}}(G,\r,a) =
  \big(
\min(a)-|G\cap\r|
\big)_+.
\end{equation}

\begin{definition}
  Let $A$ be a set of annotations and $G\subseteq \mathbb B$
  a set of breakpoint guesses. The \textbf{incomplete annotation error} is the
  count of annotation-dependent false positives and false negatives:
\begin{equation*}
  E^A_{\text{incomplete}}(G)=
    \sum_{i=1}^n 
    \hat{\textrm{FP}}(G, \r_i, a_i) + 
    \hat{\textrm{FN}}(G, \r_i, a_i).
\end{equation*}
\end{definition}



In the case of analyzing the simulated signals in the top panels of
Figure~\ref{fig:variable-density-annotation-cost}, let us consider the
set of 6 annotations $\hat A=\{(\hat a_1, \hat \r_1), \dots, (\hat
a_6, \hat \r_6)\}$ depicted using the red rectangles. These rectangles
were determined by visual inspection of the scatterplots. I used the
SegAnnDB interactive annotation web site to view the data and save a
database of 6 regions per profile \citep{SegAnnDB}. Every region $\hat
\r_i$ contains exactly 1 breakpoint, so we have $\hat a_i=\{1\}$ for every
annotation $i\in\{1, \dots, 6\}$. In real data we will probably only
be able to see a subset of the real breakpoints, but we analyze the
complete set of breakpoints in these simulated data to illustrate the
approximation induced by the annotation process.

Given any set of non-intersecting annotations $A$, we can write
$\underline r_1 < \cdots < \underline r_n$ to order the regions. Then
we can define $|A|+1$ negative annotations as
\begin{equation} 
  \label{eq:A^0}
   A^0 = \big\{ 
(0, [1,\underline r_1-1]),\ 
(0, [\overline r_1+1, \underline r_2-1]),\ 
\dots,\ 
(0, [\overline r_{n-1}+1,\underline r_n-1]),\ 
(0, [\underline r_n+1,d-1])
\big\},
\end{equation}
as drawn with yellow rectangles in the middle panels of
Figure~\ref{fig:variable-density-annotation-cost}. We will use the
complete set of annotations $\hat A\cup \hat A^0$ to define the
annotation error $E^{ \hat A\cup \hat A^0}_{\text{incomplete}}(G)$ for
breakpoint guesses $G$ given by models of these simulated signals.

\newpage

In Figure~\ref{fig:variable-density-sigerr-small}, we plot some model
selection error functions for the 2 simulated signals shown in
Figure~\ref{fig:variable-density-annotation-cost}. It is clear that
the annotation error is a good approximation of the breakpointError,
and there are several interesting observations to note. 


\begin{itemize}
\item \textbf{Signal}: in these simulated data, the true piecewise
  constant signal $\mathbf m$ is available, so an efficient model
  selection procedure \citep{sylvain-survey} would select the
  estimated model which is closest to the true signal. That idea is
  illustrated in Figure~\ref{fig:motivation-modelOnly}, and can be
  used in this context by minimizing
  \begin{equation}
    \label{eq:signal_cost}
    E_{\text{signal}}(k) = \log_{10}\left[
\frac 1 d \sum_{i=1}^d(\mathbf{\hat y}_i^k - \mathbf m_i)^2
\right].
  \end{equation}
  \begin{itemize}
  \item In Figure~\ref{fig:variable-density-sigerr-small}, for the
    signal sampled at 7 bases/probe, the minimum of the Signal error
    identifies a model with 7 segments.
  \item For the signal sampled at 374 bases/probe, the minimum of the
    error identifies a model with only 5 segments.
  \end{itemize}
\item \textbf{Breakpoint}: in these simulated data, the true
  breakpoints $B$ are available, so we can compute and minimize the
  breakpointError as a consistent model selection procedure
  \citep{sylvain-survey}. For both signals, the minimum of the
  breakpointError identifies a model with 7 segments (6
  breakpoints).
\item \textbf{Annotation}: we use a set of annotated regions to
  compute the incomplete annotation error, which also identifies a
  model with 7 segments. It is clear that the annotation error is a
  good approximation of the breakpointError. In the next section, we
  explicitly demonstrate the link between the breakpointError and the
  annotation error.
\end{itemize}
\begin{figure}[H]
  \centering
  \input{figure-variable-density-sigerr-small}
  \vskip -0.5cm
  \caption{Model selection error curves for 2 simulated
    signals. Minima are highlighted using circles.
    \protect\\
    \textbf{Signal}: the log squared error $E_{\text{signal}}$ of the
    estimated signal with respect to the
    true piecewise constant signal (see text).
    \protect\\
    \textbf{Breakpoint}: exact breakpointError $E^B_{\text{exact}}$.
    %described in Section~\ref{sec:breakpoint_error}.
    \protect\\
    \textbf{Annotation}: incomplete annotation error
    $E^{\hat A\cup \hat A^0}_{\text{incomplete}}$.
%for the annotations shown in
 %   Figure~\ref{fig:variable-density-annotation-cost}. 
}
  \label{fig:variable-density-sigerr-small}
\end{figure}

\newpage

\subsection{Link with breakpointError using complete annotation error}

\label{sec:complete}
It is clear from Figure~\ref{fig:variable-density-sigerr-small} that
the annotation error is a good approximation of the exact breakpoint
error when the annotations $A$ agree with the real
breakpoints $B$. In this section, we make this intuition precise by
showing exactly how to relax the breakpointError to obtain the
annotation error. There are two steps:
\begin{enumerate}
\item We define the complete annotation error by relaxing the
  definition of the exact \mbox{breakpointError}.
\item We show that the complete annotation error is equivalent
  to the incomplete annotation error when we have a complete set of
  annotations.
\end{enumerate}

%\subsection{The complete annotation error}

We will define the complete annotation error as a relaxation of the
exact breakpointError. Recall from
Definition~\ref{def:exact_breakpoint_cost}, 
the exact breakpointError is
$$
  {E }_{\text{exact}}^B(G) =
  \big|G\setminus(\cup R_B)\big|
 + \sum_{i=1}^{|B|}\text{FP}(G,\r_i)+\text{FN}(G,\r_i)+I(G,\r_i,\ell_i).
$$
To define the complete annotation error, we perform two relaxations:
\begin{itemize}
\item Instead of using the true breakpoints $B$ (which are unknown in
  real data) with equations (\ref{eq:R_i}) and (\ref{eq:L_i}) to
  determine a breakpoint region $\r_i$, we use the region $\hat\r_i$
  determined by visual
  inspection.
\item Rather than the piecewise affine imprecision $\ell_i$, we use
  the zero-one imprecision $\hat \ell_i$:
\begin{equation}
  \label{eq:hat_ell_i}
  \hat\ell_i(g) = 1_{g\not\in \hat \r_i}.
\end{equation}
We show this relaxation by ploting the imprecision functions $\ell_i$
and $\hat \ell_i$ in the bottom panels of
Figure~\ref{fig:variable-density-annotation-cost}.
\end{itemize}

\begin{definition}
  Assume there are $n$ breakpoints $B_1 < \dots < B_n$, and we observe
  a set of annotations $\hat A= \{(a_1, \hat\r_1), \dots, (a_n,
  \hat\r_n)\}$ each with $a_i=1$ breakpoint, such that $B_1\in \hat
  \r_1, \dots, B_n\in\hat \r_n$. The \textbf{complete annotation
    error} of a set of breakpoint guesses $G$ is the sum of false positive and
  false negative counts:
  \begin{eqnarray*}
    E_{\textrm{complete}}^{\hat A} (G)
    &=&  \Big|G\setminus(\cup \hat A)\Big| \nonumber
    + \sum_{i=1}^{|\hat A|}
    \textrm{FP}(G,\hat\r_i)+\textrm{FN}(G,\hat\r_i)+I(G,\hat\r_i,\hat\ell_i)\\
    &=&  \Big|G\setminus(\cup \hat A)\Big|
    + \sum_{(a,\hat\r)\in\hat A}
    \textrm{FP}(G,\hat\r)+\textrm{FN}(G,\hat\r).
  \end{eqnarray*}
\end{definition}

It is clear that $E^{\hat A}_{\text{complete}}$ depends on the
annotations only through their regions. In particular, the annotated
breakpoint counts $a_i=\{1\}$ are not used in this definition, since
we assumed that each region $\hat \r_i$ contains exactly 1
breakpoint. Also, since we used the zero-one imprecision for $\hat\ell_i$, the
imprecision function $I$ is always zero.

\begin{proposition}
  Let $\hat A^0$ be a set of negative annotations as in
  (\ref{eq:A^0}). Then for a set of breakpoint guesses $G$, the
  incomplete and complete annotation error functions are equivalent:
\begin{equation*}
  E_{\text{incomplete}}^{\hat A\cup \hat A^0}(G)
=
E^{\hat A}_{\text{complete}}(G).
\end{equation*}
\end{proposition}

\begin{proof}

To see the connection between the complete and incomplete annotation
error functions, first note that
\begin{eqnarray}
\nonumber  \hat{\text{FN}}(G,\r,\{1\}) 
&=&   \big(
1-|G\cap\r|
\big)_+\\
&=&
\text{FN}(G,\r), \label{eq:fn-hat}
\end{eqnarray}
and
\begin{eqnarray}
\nonumber
  \hat{\text{FP}}(G,\r,\{1\}) 
&=&
\big( 
|G\cap\r|-1
\big)_+\\
% &=&
% \begin{cases}
%   0 & \text{if }|H(G,r)|\leq 1\\
%   |H(G,r)| - 1 & \text{if }|H(G,r)|\geq 1
% \end{cases}\\
&=& \text{FP}(G,\r). \label{eq:fp-hat}
\end{eqnarray}
For the complete annotation error we quantified the false positive
rate of the breakpoints that fall outside of the breakpoint regions
$\hat A$ using $G\setminus(\cup \hat A)$. For the incomplete
annotation error, we instead created a set of 0-breakpoint annotations
$\hat A^0$ for this purpose. Note that by construction of the negative
regions in (\ref{eq:A^0}), we have 
\begin{equation}
  \label{eq:Rcomplement}
  G\setminus \left(\cup \hat A\right)
 = 
G\cap\left(\cup \hat A^0 \right),
\end{equation}
or in words, the guesses outside of the breakpoint annotations $\hat A$
are in the negative annotations $\hat A^0$. So using
(\ref{eq:Rcomplement}), we have
\begin{eqnarray}
  \hat{\text{FP}}(G,(\cup \hat A^0),\{0\})
&=& 
|G\cap(
  \cup \hat A^0
)|\nonumber
\\
&=&
|G\setminus(\cup\hat A)|,\label{eq:fp-outside}
\end{eqnarray}
which is the first component of the complete annotation error.

Recall that $\hat A$ represents annotated regions that each contain
exactly 1 breakpoint, and $\hat A^0$ are regions with no breakpoints. So
using (\ref{eq:fn-hat}), (\ref{eq:fp-hat}), and
(\ref{eq:fp-outside}), we have that the incomplete annotation error
is equivalent to the complete error:
\begin{eqnarray}
  E_{\text{incomplete}}^{\hat A\cup \hat A^0}(G)
&=&
\sum_{(a, \r)\in \hat A^0} \hat{\text{FP}}(G,\r,a)\nonumber 
+
\sum_{(a, \r)\in\hat A} \hat{\text{FP}}(G,\r,a) + \hat{\text{FN}}(G,\r,a)
 \\
&=&
 \hat{\text{FP}}(G,\cup \hat A^0,\{0\})\nonumber 
+
\sum_{(a, \r)\in\hat A} \hat{\text{FP}}(G,\r,\{1\}) + \hat{\text{FN}}(G,\r,\{1\})
 \\
&=&
|G\setminus(\cup\hat A)|
+\nonumber
\sum_{(a, \r)\in\hat A} {\text{FP}}(G,\r) + {\text{FN}}(G,\r)\\
&=&
E^{\hat A}_{\text{complete}}(G).
\end{eqnarray}

\end{proof}

So in fact the incomplete annotation error is equivalent to the
complete error when the annotated regions $\hat A$ each contain
exactly 1 breakpoint. But we call this the incomplete error since it
is also well-defined for arbitrary sets of regions $A$.

%This result is significant since it establishes a solid co

\newpage

\subsection{Zero-one annotation error}
\label{sec:zero-one}

The incomplete annotation error counts incorrect breakpoints. In this
section, we show that by thresholding the incomplete annotation error,
we can obtain the zero-one annotation error function. This is the
original annotation error function that was introduced by
\citet{HOCKING-breakpoints}, who used it to count the number of
incorrect regions.

First, let us define the zero-one thresholding function
$t:\mathbb Z^+\rightarrow\mathbb Z^+$ as
\begin{equation}
  \label{eq:thresholding}
  t(x)=1_{x\neq 0} =
  \begin{cases}
    1 & \text{if }x\neq 0\\
    0 & \text{otherwise}.
  \end{cases}
\end{equation}

The idea of thresholding is to limit the error that any one annotation
can induce. We define the zero-one annotation error as
\begin{eqnarray}
  \label{eq:ann01err}
  E_{01}^{A}(G)
&=&\nonumber
 \sum_{(a, \r)\in A} 
t\left[\hat{\text{FP}}(G,\r,a)\right]+
t\left[\hat{\text{FN}}(G,\r,a)\right]\\
&=&\nonumber
 \sum_{(a, \r)\in A} 
1_{|G\cap\r|>\max(a)}+
1_{|G\cap \r|<\min(a)}\\
&=&
 \sum_{(a, \r)\in A} 
1_{|G\cap\r|\not\in a}.
\end{eqnarray}
So using the zero-one annotation error, we count incorrect annotated
regions instead of incorrect breakpoint guesses.  

\newpage

\subsection{Comparing annotation error functions}

In practice, we have few annotated regions per signal in real data. In
Figure~\ref{fig:variable-density-sigerr}, we show how the annotation
error is degraded as we remove annotations. In particular, it is clear
that using the thresholded zero-one annotation error significantly
degrades the approximation of the FP curve. Nevertheless, it is worth
noting that minimum of the zero-one error still uniquely identifies
the correct model with 7 segments. Even after removing many
annotations, the minimum error still identifies the correct
model, but not uniquely. 

% \begin{itemize}
% \item \textbf{Complete}: the annotation error is a
%   close approximation of the breakpoint error, and its minimum also
%   identifies the correct model.
% \item \textbf{Zero-one}: thresholding degrades the approximation of FP
%   on models with many breakpoints. However, the minimum error still
%   identifies the correct model.
% \item \textbf{Incomplete}: removing half of the annotations further degrades
%   the approximation.
%   \begin{itemize}
% \item However, for the signal sampled at 374 bases/probe, the
%   minimum annotation error still uniquely identifies the correct
%   model. 
% \item For the signal sampled at 7 bases/probe, the
%   $\hat{\text{FP}}$ is a poor approximation of $\text{FP}$ because
%   there are very large regions with no breakpoints and no annotations.
%   \end{itemize}
% \item \textbf{Positive}: the annotation error is formed using only 3
%   annotated breakpoint regions. The minimum annotation error
%   identifies the correct model, but not uniquely.
% \end{itemize}




\begin{figure}[H]
  %\hspace{-1.5cm}
  \input{figure-variable-density-sigerr}
  \caption{Comparison of annotation error functions as the set of
    annotations changes. Minima are highlighted using circles.
    \protect\\
    \textbf{Complete}: annotation error
    %$E_{\text{incomplete}}^{\hat A\cup \hat A^0}$ 
    for a complete set of 6 positive and 7 negative annotations.
    \protect\\
    \textbf{Zero-one}: zero-one annotation error 
    %$E_{01}^{\hat A\cup \hat A^0}$
    for a complete set of 6 positive and 7 negative annotations.
    \protect\\
    \textbf{Incomplete}: zero-one annotation error 
    %$E_{01}^{\hat R,A}$
    for 3 positive and 4 negative annotations.
    \protect\\
    \textbf{Positive}: zero-one annotation error 
    %%$E_{01}^{\hat R,A}$
    for 3 positive annotations.}
  \label{fig:variable-density-sigerr}
\end{figure}

In conclusion, this section has discussed the connections between the
breakpointError and the annotation error functions. Whereas the
breakpointError is computable only when the true set of breakpoints is
known (e.g. simulated data), the annotation error is readily
computable in any data set using a set of visually determined
annotations. We showed that if the annotations are consistent with the
true breakpoints, then the annotation error function is a good
approximation of the breakpointError
(Figure~\ref{fig:variable-density-sigerr-small}). Finally, we observed
that even after thresholding and removing annotations, the annotation
error function can still be used to identify a set of minimum error
segmentation models (Figure~\ref{fig:variable-density-sigerr}).

%\newpage

\section{Conclusions and future work}

In this paper we defined the breakpointError, which can be used to
quantify the breakpoint detection accuracy of a segmentation model,
when the true breakpoint positions are known. In Section~4 we showed
one application of the breakpointError for determining optimal penalty
constants in several simulated data sets. In Section~5 we discussed
the relationship of the breakpointError to the annotation error, which
has been used for supervised segmentation of real data sets
\citep{HOCKING-breakpoints, HOCKING-penalties, SegAnnDB}. We showed
that the annotation error is a good approximation of the
breakpointError when the annotated regions agree with the true
breakpoints. This provides some justification for using the annotation
error in supervised analysis of real data sets.

For future work, it will be interesting to apply the breakpointError
to more realistic tasks. For example, \citet{perf-eval-framework}
proposed to evaluate breakpoint detection algorithms by adding
breakpoints and noise to real data sets. In their framework, the true
breakpoint positions are known, and a region around each breakpoint is
used to quantify the number of true and false positive breakpoint
detections. Instead of using the zero-one loss with an arbitrarily
sized region, the breakpointError could be used to more precisely
quantify breakpoint estimates, since it counts imprecision
(\ref{eq:imprecision}) in addition to false positive and false
negative breakpoint detections.

TODO: compare with V-measure \citep{vmeasure}.

To facilitate the use of the breakpointError in future work, it is
implemented in the R package \verb|breakpointError| on R-Forge. It can
be installed in R using

\begin{verbatim}
install.packages("breakpointError", repos="http://r-forge.r-project.org")
\end{verbatim}

\textbf{Acknowledgements}: Thanks to Marco Cuturi for references about
distance functions for comparing probability distributions. Thanks to
Guillem Rigaill for helpful comments on a preliminary version of this
paper.

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
